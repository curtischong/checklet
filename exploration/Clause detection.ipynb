{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d214d773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This          DET   <════╗                                     det\n",
      "all           DET   <══╗ ║                                     det\n",
      "encompassing  VERB  <╗ ║ ║                                     amod\n",
      "experience    NOUN  ═╝═╝═╝<════════════════╗                   nsubj\n",
      "wore          VERB  ═╗═════╗═════════════╗═╝═╗═══════════════╗ ROOT\n",
      "off           ADP   <╝     ║             ║   ║               ║ prt\n",
      "for           ADP   ═══╗═╗<╝             ║   ║               ║ prep\n",
      "a             DET   <╗ ║ ║               ║   ║               ║ det\n",
      "moment        NOUN  ═╝<╝ ║               ║   ║               ║ pobj\n",
      "and           CCONJ <════╝               ║   ║               ║ cc\n",
      "in            ADP   ═══╗<════════════╗   ║   ║               ║ prep\n",
      "that          DET   <╗ ║             ║   ║   ║               ║ det\n",
      "moment        NOUN  ═╝<╝             ║   ║   ║               ║ pobj\n",
      ",             PUNCT <════════════════║═╗ ║   ║               ║ punct\n",
      "my            PRON  <╗               ║ ║ ║   ║               ║ poss\n",
      "awareness     NOUN  ═╝<════════════╗ ║ ║ ║   ║               ║ nsubj\n",
      "came          VERB  ═════════════╗═╝═╝═╝ ║<══╝               ║ conj\n",
      "gasping       VERB  ═══════════╗<╝       ║                   ║ advcl\n",
      "to            ADP   ═════════╗<╝         ║                   ║ prep\n",
      "the           DET   <══════╗ ║           ║                   ║ det\n",
      "surface       NOUN  ═════╗═╝<╝           ║                   ║ pobj\n",
      "of            ADP   ═══╗<╝               ║                   ║ prep\n",
      "the           DET   <╗ ║                 ║                   ║ det\n",
      "hallucination NOUN  ═╝<╝                 ║                   ║ pobj\n",
      "and           CCONJ <════════════════════╝                   ║ cc\n",
      "I             PRON  <══════════════════════════════════╗     ║ nsubj\n",
      "was           AUX   ═══════════════════════════════╗═╗═╝═╗═╗<╝ conj\n",
      "able          ADJ   ═════════════════════════════╗<╝ ║   ║ ║   acomp\n",
      "to            PART  <══════════════════════════╗ ║   ║   ║ ║   aux\n",
      "consider      VERB  ═╗═══════════════════════╗═╝<╝   ║   ║ ║   xcomp\n",
      "momentarily   ADV   <╝                       ║       ║   ║ ║   advmod\n",
      "that          SCONJ <══════════════════════╗ ║       ║   ║ ║   mark\n",
      "I             PRON  <════════════════════╗ ║ ║       ║   ║ ║   nsubj\n",
      "had           AUX   <══════════════════╗ ║ ║ ║       ║   ║ ║   aux\n",
      "killed        VERB  ═╗═══════════════╗═╝═╝═╝<╝       ║   ║ ║   ccomp\n",
      "myself        PRON  <╝               ║               ║   ║ ║   dobj\n",
      "by            ADP   ═══════════════╗<╝               ║   ║ ║   prep\n",
      "taking        VERB  ═════════════╗<╝                 ║   ║ ║   pcomp\n",
      "an            DET   <══════════╗ ║                   ║   ║ ║   det\n",
      "outrageous    ADJ   <════════╗ ║ ║                   ║   ║ ║   amod\n",
      "dose          NOUN  ═══════╗═╝═╝<╝                   ║   ║ ║   dobj\n",
      "of            ADP   ═════╗<╝                         ║   ║ ║   prep\n",
      "an            DET   <══╗ ║                           ║   ║ ║   det\n",
      "online        ADJ   <╗ ║ ║                           ║   ║ ║   amod\n",
      "drug          NOUN  ═╝═╝<╝                           ║   ║ ║   pobj\n",
      "and           CCONJ <════════════════════════════════╝   ║ ║   cc\n",
      "this          PRON  <════════════╗                       ║ ║   nsubj\n",
      "was           AUX   ═══════════╗═╝<══════════════════════╝ ║   conj\n",
      "the           DET   <════════╗ ║                           ║   det\n",
      "most          ADV   <╗       ║ ║                           ║   advmod\n",
      "pathetic      ADJ   ═╝<════╗ ║ ║                           ║   amod\n",
      "death         NOUN  <╗     ║ ║ ║                           ║   compound\n",
      "experience    NOUN  ═╝═══╗═╝═╝<╝                           ║   attr\n",
      "of            ADP   ═══╗<╝                                 ║   prep\n",
      "all           DET   <╗ ║                                   ║   det\n",
      "time          NOUN  ═╝<╝                                   ║   pobj\n",
      ".             PUNCT <══════════════════════════════════════╝   punct\n",
      "This all encompassing experience wore off for a moment and and\n",
      "\n",
      "in that moment , my awareness came gasping to the surface of the hallucination\n",
      "\n",
      "I was able to consider momentarily that I had killed myself by taking an outrageous dose of an online drug and this was the most pathetic death experience of all time .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/65227103/clause-extraction-long-sentence-segmentation-in-python\n",
    "\n",
    "\n",
    "import spacy\n",
    "import deplacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"This all encompassing experience wore off for a moment and in that moment, my awareness came gasping to the surface of the hallucination and I was able to consider momentarily that I had killed myself by taking an outrageous dose of an online drug and this was the most pathetic death experience of all time.\"\n",
    "\n",
    "doc = en(text)\n",
    "deplacy.render(doc)\n",
    "\n",
    "seen = set() # keep track of covered words\n",
    "\n",
    "chunks = []\n",
    "for sent in doc.sents:\n",
    "    heads = [cc for cc in sent.root.children if cc.dep_ == 'conj']\n",
    "\n",
    "    for head in heads:\n",
    "        words = [ww for ww in head.subtree]\n",
    "        for word in words:\n",
    "            seen.add(word)\n",
    "        chunk = (' '.join([ww.text for ww in words]))\n",
    "        chunks.append( (head.i, chunk) )\n",
    "\n",
    "    unseen = [ww for ww in sent if ww not in seen]\n",
    "    chunk = ' '.join([ww.text for ww in unseen])\n",
    "    chunks.append( (sent.root.i, chunk) )\n",
    "\n",
    "chunks = sorted(chunks, key=lambda x: x[0])\n",
    "\n",
    "for ii, chunk in chunks:\n",
    "    print(chunk, end=\"\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf2f37",
   "metadata": {},
   "source": [
    "### Stanford Core NLP\n",
    "download from https://stanfordnlp.github.io/CoreNLP/\n",
    "\n",
    "then run \n",
    "\n",
    "`cd /Users/curtis/Desktop/idioms/pkg/stanford-corenlp-4.1.0;\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a62a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : \n",
      " This all encompassing experience wore off for a moment and in that moment, my awareness came gasping to the surface of the hallucination and I was able to consider momentarily that I had killed myself by taking an outrageous dose of an online drug and this was the most pathetic death experience of all time.\n",
      "-This . was the most pathetic death experience of all time\n",
      "\n",
      "-all encompassing experience in that moment , my awareness and was able to consider momentarily\n",
      "\n",
      "-all encompassing experience in that moment , my awareness and came gasping to the surface of the hallucination\n",
      "\n",
      "-all encompassing experience in that moment , my awareness and wore off for a moment\n",
      "\n",
      "-I had killed myself by taking an outrageous dose of an online drug and this\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from https://github.com/rahulkg31/sentence-to-clauses/blob/master/sent_to_clauses.py\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tree import ParentedTree\n",
    "from pycorenlp import *\n",
    "nlp=StanfordCoreNLP(\"http://0.0.0.0:9000/\")\n",
    "import json\n",
    "\n",
    "# get verb phrases\n",
    "# if one \"VP\" node has 2 or more \"VP\" children then\n",
    "# all child \"VP\" while be used as verb phrases\n",
    "# since a clause may have more than one verb phrases\n",
    "# ex:- he plays cricket but does not play hockey\n",
    "# here two verb phrases are \"plays cricket\" and \"does not play hockey\"\n",
    "#                       ROOT\n",
    "#                        |\n",
    "#                        S\n",
    "#   _____________________|____\n",
    "#  |                          VP\n",
    "#  |          ________________|____\n",
    "#  |         |           |         VP\n",
    "#  |         |           |     ____|________\n",
    "#  |         VP          |    |    |        VP\n",
    "#  |     ____|_____      |    |    |    ____|____\n",
    "#  NP   |          NP    |    |    |   |         NP\n",
    "#  |    |          |     |    |    |   |         |\n",
    "# PRP  VBZ         NN    CC  VBZ   RB  VB        NN\n",
    "#  |    |          |     |    |    |   |         |\n",
    "#  he plays     cricket but  does not play     hockey\n",
    "def get_verb_phrases(t):\n",
    "    verb_phrases = []\n",
    "    num_children = len(t)\n",
    "    num_VP = sum(1 if t[i].label() == \"VP\" else 0 for i in range(0, num_children))\n",
    "\n",
    "    if t.label() != \"VP\":\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].height() > 2:\n",
    "                verb_phrases.extend(get_verb_phrases(t[i]))\n",
    "    elif t.label() == \"VP\" and num_VP > 1:\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].label() == \"VP\":\n",
    "                if t[i].height() > 2:\n",
    "                    verb_phrases.extend(get_verb_phrases(t[i]))\n",
    "    else:\n",
    "        verb_phrases.append(' '.join(t.leaves()))\n",
    "\n",
    "    return verb_phrases\n",
    "\n",
    "# get position of first node \"VP\" while traversing from top to bottom\n",
    "# get the position of subordinating conjunctions like after, as, before, if, since, while etc\n",
    "# delete the node at these positions to get the subject\n",
    "# first delete vp nodes then subordinating conjunction nodes\n",
    "# ie, get the part without verb phrases\n",
    "# in the above example \"he\" will be returned\n",
    "def get_pos(t):\n",
    "    vp_pos = []\n",
    "    sub_conj_pos = []\n",
    "    num_children = len(t)\n",
    "    children = [t[i].label() for i in range(0,num_children)]\n",
    "\n",
    "    flag = re.search(r\"(S|SBAR|SBARQ|SINV|SQ)\", ' '.join(children))\n",
    "\n",
    "    if \"VP\" in children and not flag:\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].label() == \"VP\":\n",
    "                vp_pos.append(t[i].treeposition())\n",
    "    elif not \"VP\" in children and not flag:\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].height() > 2:\n",
    "                temp1,temp2 = get_pos(t[i])\n",
    "                vp_pos.extend(temp1)\n",
    "                sub_conj_pos.extend(temp2)\n",
    "    # comment this \"else\" part, if want to include subordinating conjunctions\n",
    "    else:\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].label() in [\"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"]:\n",
    "                temp1, temp2 = get_pos(t[i])\n",
    "                vp_pos.extend(temp1)\n",
    "                sub_conj_pos.extend(temp2)\n",
    "            else:\n",
    "                sub_conj_pos.append(t[i].treeposition())\n",
    "\n",
    "    return (vp_pos,sub_conj_pos)\n",
    "\n",
    "\n",
    "# get all clauses\n",
    "def get_clause_list(sent):\n",
    "    parser = nlp.annotate(sent, properties={\"annotators\":\"parse\",\"outputFormat\": \"json\"})\n",
    "    parser=json.loads(parser)\n",
    "    #print(parser[\"sentences\"][0][\"parse\"])\n",
    "    sent_tree = ParentedTree.fromstring(parser[\"sentences\"][0][\"parse\"])\n",
    "    clause_level_list = [\"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"]\n",
    "    clause_list = []\n",
    "    sub_trees = []\n",
    "    # sent_tree.pretty_print()\n",
    "\n",
    "    # break the tree into subtrees of clauses using\n",
    "    # clause levels \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"\n",
    "    for sub_tree in reversed(list(sent_tree.subtrees())):\n",
    "        if sub_tree.label() in clause_level_list:\n",
    "            if sub_tree.parent().label() in clause_level_list:\n",
    "                continue\n",
    "\n",
    "            if (len(sub_tree) == 1 and sub_tree.label() == \"S\" and sub_tree[0].label() == \"VP\"\n",
    "                and not sub_tree.parent().label() in clause_level_list):\n",
    "                continue\n",
    "\n",
    "            sub_trees.append(sub_tree)\n",
    "            del sent_tree[sub_tree.treeposition()]\n",
    "\n",
    "    # for each clause level subtree, extract relevant simple sentence\n",
    "    for t in sub_trees:\n",
    "        # get verb phrases from the new modified tree\n",
    "        verb_phrases = get_verb_phrases(t)\n",
    "\n",
    "        # get tree without verb phrases (mainly subject)\n",
    "        # remove subordinating conjunctions\n",
    "        vp_pos,sub_conj_pos = get_pos(t)\n",
    "        for i in vp_pos:\n",
    "            del t[i]\n",
    "        for i in sub_conj_pos:\n",
    "            del t[i]\n",
    "\n",
    "        subject_phrase = ' '.join(t.leaves())\n",
    "\n",
    "        # update the clause_list\n",
    "        for i in verb_phrases:\n",
    "            clause_list.append(subject_phrase + \" \" + i)\n",
    "\n",
    "    clause_list.reverse()\n",
    "    return clause_list\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    # sent = \"he plays cricket but does not play hockey\"\n",
    "    # sent = re.sub(r\"(\\.|,|\\?|\\(|\\)|\\[|\\])\",\" \",sent)\n",
    "    # clause_qlist = get_clause_list(sent)\n",
    "    # print(clause_list)\n",
    "while (True):\n",
    "    sent = input(\"sentence : \\n \")\n",
    "    #sent = re.sub(r\"(\\.|,|\\?|\\(|\\)|\\[|\\])\", \" \", sent)\n",
    "    for clause in get_clause_list(sent):\n",
    "        print(\"-\" + clause,end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85a7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
