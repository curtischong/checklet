{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "glove_dimensions = 300\n",
    "\n",
    "\n",
    "def load_glove():\n",
    "    n = 400000  # I opened the file in vscode to get this number\n",
    "    # https://nlp.stanford.edu/projects/glove/\n",
    "    glove_EMBEDDING_FILE = open('datasets/glove.6B/glove.6B.300d.txt')\n",
    "    words = [\"\"] * n\n",
    "    embeddings = np.empty([n, glove_dimensions])\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    i = 0\n",
    "    for row in glove_EMBEDDING_FILE:\n",
    "        word, embedding = get_coefs(*row.split(\" \"))\n",
    "        words[i] = word\n",
    "        embeddings[i] = embedding\n",
    "        i += 1\n",
    "    return words, embeddings\n",
    "\n",
    "\n",
    "# https://fasttext.cc/docs/en/english-vectors.html\n",
    "# wiki-news-300d-1M.vec.zip\n",
    "wiki_dimensions = 300\n",
    "\n",
    "\n",
    "def load_wiki_news():\n",
    "    n = 1000000\n",
    "    wiki_EMBEDDING_FILE = open('datasets/wiki-news-300d-1M.vec')\n",
    "    words = [\"\"] * n\n",
    "    embeddings = np.empty([n, wiki_dimensions])\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    i = 0\n",
    "    for row in tqdm(wiki_EMBEDDING_FILE):\n",
    "        word, embedding = get_coefs(*row.split(\" \"))\n",
    "        if len(embedding) != wiki_dimensions:\n",
    "            print(\"skip\")\n",
    "            continue\n",
    "        words[i] = word\n",
    "        embeddings[i] = embedding\n",
    "        i += 1\n",
    "    return words, embeddings\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/datasets/ranik40/paragram-300-sl999?resource=download\n",
    "para_dimensions = 300\n",
    "\n",
    "\n",
    "def load_paragram():\n",
    "    n = 66199  # I opened the file in vscode to get this number\n",
    "    para_EMBEDDING_FILE = open('datasets/paragram_300_sl999.txt')\n",
    "    words = [\"\"] * n\n",
    "    embeddings = np.empty([n, wiki_dimensions])\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    i = 0\n",
    "    for row in tqdm(para_EMBEDDING_FILE):\n",
    "        word, embedding = get_coefs(*row.split(\" \"))\n",
    "        if len(embedding) != wiki_dimensions:\n",
    "            print(\"skip\")\n",
    "            continue\n",
    "        words[i] = word\n",
    "        embeddings[i] = embedding\n",
    "        i += 1\n",
    "    return words, embeddings\n",
    "\n",
    "\n",
    "#all_dimensions = glove_dimensions + wiki_dimensions + para_dimensions\n",
    "all_dimensions = 300  # adding the emeddings is fine cause https://randorithms.com/2020/11/17/Adding-Embeddings.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "#glove_words, glove_embeddings = load_glove()\n",
    "#np.save('embeddings/glove_embeddings.npy', glove_embeddings)\n",
    "#np.save('embeddings/glove_words.npy', glove_words)\n",
    "#wiki_words, wiki_embeddings = load_wiki_news()\n",
    "#np.save('embeddings/wiki_embeddings.npy', wiki_embeddings)\n",
    "#np.save('embeddings/wiki_words.npy', wiki_words)\n",
    "#para_words, para_embeddings = load_paragram()\n",
    "#np.save('embeddings/para_embeddings.npy', para_embeddings)\n",
    "#np.save('embeddings/para_words.npy', para_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1229131/1229131 [00:13<00:00, 89263.91it/s] \n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = np.load('embeddings/glove_embeddings.npy')\n",
    "glove_words = np.load('embeddings/glove_words.npy')\n",
    "wiki_embeddings = np.load('embeddings/wiki_embeddings.npy')\n",
    "wiki_words = np.load('embeddings/wiki_words.npy')\n",
    "para_embeddings = np.load('embeddings/para_embeddings.npy')\n",
    "para_words = np.load('embeddings/para_words.npy')\n",
    "glove_dict = dict(zip(glove_words, glove_embeddings))\n",
    "wiki_dict = dict(zip(wiki_words, wiki_embeddings))\n",
    "para_dict = dict(zip(para_words, para_embeddings))\n",
    "words = np.array(list(set(np.concatenate([glove_words, wiki_words, para_words]))))\n",
    "n = words.size\n",
    "embeddings = [\"\"] * n\n",
    "for i in tqdm(range(n)):\n",
    "    word = words[i]\n",
    "    vec = np.zeros([all_dimensions])\n",
    "    if word in glove_dict:\n",
    "        #vec += np.concatenate([glove_dict[word], np.zeros(600)])\n",
    "        vec += glove_dict[word]\n",
    "    if word in wiki_dict:\n",
    "        #vec += np.concatenate([np.zeros(300), wiki_dict[word], np.zeros(300)])\n",
    "        vec += wiki_dict[word]\n",
    "    if word in para_dict:\n",
    "        #vec += np.concatenate([np.zeros(600), para_dict[word]])\n",
    "        vec += para_dict[word]\n",
    "    embeddings[i] = vec\n",
    "\n",
    "np.save('embeddings/all_embeddings.npy', embeddings)\n",
    "np.save('embeddings/all_words.npy', words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "embeddings = np.load('embeddings/all_embeddings.npy')\n",
    "words = np.load('embeddings/all_words.npy')\n",
    "word_to_vec = dict(zip(words, embeddings))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_k_closest_words(query_vector, words, matrix, k):\n",
    "    scores = matrix.dot(query_vector)\n",
    "    best_indexes = np.argpartition(scores, -k)[-k:]\n",
    "    #best_indexes = np.argmax(res)\n",
    "    best_words = np.take(words, best_indexes)\n",
    "    best_scores = np.take(scores, best_indexes)\n",
    "    ans = list(zip(best_scores, best_words))\n",
    "    ans.sort(reverse=True)\n",
    "    return [word for _, word in ans]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['love', 'loves', 'loving', 'lover', 'loved']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector = word_to_vec[\"love\"]\n",
    "get_k_closest_words(query_vector, words, embeddings, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.16255710034245\n"
     ]
    },
    {
     "data": {
      "text/plain": "['organized', 'led', 'organised', 'conducted', 'headed']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_for_words(qwords, word_to_vec):\n",
    "    nwords = len(qwords)\n",
    "    ans = np.zeros([all_dimensions])\n",
    "    for word in qwords:\n",
    "        ans += word_to_vec[word]\n",
    "    print(np.linalg.norm(ans))\n",
    "    return ans / nwords\n",
    "\n",
    "\n",
    "qvector = query_for_words([\"organized\", \"led\"], word_to_vec)\n",
    "\n",
    "#print(query_vector)\n",
    "get_k_closest_words(qvector, words, embeddings, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}