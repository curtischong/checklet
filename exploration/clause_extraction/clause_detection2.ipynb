{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d214d773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 inserting                                                                      \n",
      "     ________________________________|______________________________________________________________             \n",
      "    |      |   |   |   |                   categorizing                                             |           \n",
      "    |      |   |   |   |     ___________________|_____________________                              |            \n",
      "    |      |   |   |   |    |        |          |                     by                            |           \n",
      "    |      |   |   |   |    |        |          |                     |                             |            \n",
      "    |      |   |   |   |    |        |          |                  creating                      deployed       \n",
      "    |      |   |   |   |    |        |          |              _______|_________             _______|_______     \n",
      "    |      |   |   |   |    |       from        |           email               |        service            |   \n",
      "    |      |   |   |   |    |        |          |         ____|_______          |      _____|_______        |    \n",
      "    |      |   |   |   |    |     clients       in       |        monitoring    |     |             on      |   \n",
      "    |      |   |   |   |    |        |          |        |            |         |     |             |       |    \n",
      "    |      |   |   |   |  emails    200        CRM       |           API      using   |           Cloud    with \n",
      "    |      |   |   |   |    |        |          |        |            |         |     |             |       |    \n",
      "Automated and  ,  and  .   20k+     over        a        an          REST    Express the          Azure   Docker\n",
      "\n",
      "2\n",
      "3 categorizing 20k+ emails from over 200 clients in a CRM by creating an email monitoring REST API using Express\n",
      "\n",
      "24 deployed the service on Azure Cloud with Docker\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/65227103/clause-extraction-long-sentence-segmentation-in-python\n",
    "\n",
    "\n",
    "import spacy\n",
    "import deplacy\n",
    "from nltk import Tree\n",
    "en = spacy.load('en_core_web_md')\n",
    "\n",
    "#text = \"This all encompassing experience wore off for a moment and in that moment, my awareness came gasping to the surface of the hallucination and I was able to consider momentarily that I had killed myself by taking an outrageous dose of an online drug and this was the most pathetic death experience of all time.\"\n",
    "text = \"Automated inserting and categorizing 20k+ emails from over 200 clients in a CRM by creating an email monitoring REST API using Express, and deployed the service on Azure Cloud with Docker.\"\n",
    "#text = \"he plays cricket but does not play hockey.\"\n",
    "\n",
    "doc = en(text)\n",
    "#deplacy.render(doc)\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "\n",
    "seen = set() # keep track of covered words\n",
    "\n",
    "chunks = []\n",
    "for sent in doc.sents:\n",
    "    #[print(str(x) + \"~\") for x in sent.root.children]\n",
    "    heads = [cc for cc in sent.root.children if cc.dep_ in ['conj', 'adp']]\n",
    "    print(len(heads))\n",
    "\n",
    "    for head in heads:\n",
    "        words = [ww for ww in head.subtree]\n",
    "        for word in words:\n",
    "            seen.add(word)\n",
    "        chunk = (' '.join([ww.text for ww in words]))\n",
    "        chunks.append( (head.i, chunk) )\n",
    "\n",
    "    # this last piece of code is a really lazy way to print the remaining chars and\n",
    "    # saying that it's a clause, when in fact it isn't\n",
    "    #unseen = [ww for ww in sent if ww not in seen]\n",
    "    #chunk = ' '.join([ww.text for ww in unseen])\n",
    "    #chunks.append( (sent.root.i, chunk) )\n",
    "\n",
    "chunks = sorted(chunks, key=lambda x: x[0])\n",
    "\n",
    "for ii, chunk in chunks:\n",
    "    print(ii, chunk, end=\"\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cc06ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                plays                 \n",
      "  ________________|_________           \n",
      " |     |     |    |        play       \n",
      " |     |     |    |     ____|_____     \n",
      " he cricket but   .   does not  hockey\n",
      "\n",
      "1\n",
      "6 does not play hockey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/65227103/clause-extraction-long-sentence-segmentation-in-python\n",
    "\n",
    "\n",
    "import spacy\n",
    "import deplacy\n",
    "from nltk import Tree\n",
    "en = spacy.load('en_core_web_md')\n",
    "\n",
    "#text = \"This all encompassing experience wore off for a moment and in that moment, my awareness came gasping to the surface of the hallucination and I was able to consider momentarily that I had killed myself by taking an outrageous dose of an online drug and this was the most pathetic death experience of all time.\"\n",
    "#text = \"Automated inserting and categorizing 20k+ emails from over 200 clients in a CRM by creating an email monitoring REST API using Express, and deployed the service on Azure Cloud with Docker.\"\n",
    "text = \"he plays cricket but does not play hockey.\"\n",
    "\n",
    "doc = en(text)\n",
    "#deplacy.render(doc)\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "\n",
    "seen = set() # keep track of covered words\n",
    "\n",
    "chunks = []\n",
    "for sent in doc.sents:\n",
    "    #[print(str(x) + \"~\") for x in sent.root.children]\n",
    "    heads = [cc for cc in sent.root.children if cc.dep_ in ['conj']]\n",
    "    print(len(heads))\n",
    "\n",
    "    for head in heads:\n",
    "        words = [ww for ww in head.subtree]\n",
    "        for word in words:\n",
    "            seen.add(word)\n",
    "        chunk = (' '.join([ww.text for ww in words]))\n",
    "        chunks.append( (head.i, chunk) )\n",
    "\n",
    "chunks = sorted(chunks, key=lambda x: x[0])\n",
    "\n",
    "for ii, chunk in chunks:\n",
    "    print(ii, chunk, end=\"\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da293b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b55c53dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ADP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf2f37",
   "metadata": {},
   "source": [
    "### Stanford Core NLP\n",
    "download from https://stanfordnlp.github.io/CoreNLP/\n",
    "\n",
    "then run \n",
    "\n",
    "`cd /Users/curtis/Desktop/idioms/pkg/stanford-corenlp-4.1.0;\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdd11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/rahulkg31/sentence-to-clauses/blob/master/sent_to_clauses.py\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tree import ParentedTree\n",
    "from pycorenlp import *\n",
    "# start the connection here\n",
    "nlp=StanfordCoreNLP(\"http://0.0.0.0:9000/\")\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "600a62a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num clauses: 3\n",
      "- deployed the service on Azure Cloud with Docker\n",
      "\n",
      "- REST API using Express\n",
      "\n",
      "- inserting and categorizing 20k emails from over 200 clients in a CRM by creating an email monitoring\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get verb phrases\n",
    "# if one \"VP\" node has 2 or more \"VP\" children then\n",
    "# all child \"VP\" while be used as verb phrases\n",
    "# since a clause may have more than one verb phrases\n",
    "# ex:- he plays cricket but does not play hockey\n",
    "# here two verb phrases are \"plays cricket\" and \"does not play hockey\"\n",
    "#                       ROOT\n",
    "#                        |\n",
    "#                        S\n",
    "#   _____________________|____\n",
    "#  |                          VP\n",
    "#  |          ________________|____\n",
    "#  |         |           |         VP\n",
    "#  |         |           |     ____|________\n",
    "#  |         VP          |    |    |        VP\n",
    "#  |     ____|_____      |    |    |    ____|____\n",
    "#  NP   |          NP    |    |    |   |         NP\n",
    "#  |    |          |     |    |    |   |         |\n",
    "# PRP  VBZ         NN    CC  VBZ   RB  VB        NN\n",
    "#  |    |          |     |    |    |   |         |\n",
    "# 1\n",
    "def get_verb_phrases(t):\n",
    "    verb_phrases = []\n",
    "    num_children = len(t)\n",
    "    num_VP = sum(1 if t[i].label() == \"VP\" else 0 for i in range(0, num_children))\n",
    "\n",
    "    if t.label() != \"VP\":\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].height() > 2:\n",
    "                verb_phrases.extend(get_verb_phrases(t[i]))\n",
    "    elif t.label() == \"VP\" and num_VP > 1:\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].label() == \"VP\":\n",
    "                if t[i].height() > 2:\n",
    "                    verb_phrases.extend(get_verb_phrases(t[i]))\n",
    "    else:\n",
    "        verb_phrases.append(' '.join(t.leaves()))\n",
    "\n",
    "    return verb_phrases\n",
    "\n",
    "# get position of first node \"VP\" while traversing from top to bottom\n",
    "# get the position of subordinating conjunctions like after, as, before, if, since, while etc\n",
    "# delete the node at these positions to get the subject\n",
    "# first delete vp nodes then subordinating conjunction nodes\n",
    "# ie, get the part without verb phrases\n",
    "# in the above example \"he\" will be returned\n",
    "def get_pos(t):\n",
    "    vp_pos = []\n",
    "    sub_conj_pos = []\n",
    "    num_children = len(t)\n",
    "    children = [t[i].label() for i in range(0,num_children)]\n",
    "\n",
    "    flag = re.search(r\"(S|SBAR|SBARQ|SINV|SQ)\", ' '.join(children))\n",
    "\n",
    "    if \"VP\" in children and not flag:\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].label() == \"VP\":\n",
    "                vp_pos.append(t[i].treeposition())\n",
    "    elif not \"VP\" in children and not flag:\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].height() > 2:\n",
    "                temp1,temp2 = get_pos(t[i])\n",
    "                vp_pos.extend(temp1)\n",
    "                sub_conj_pos.extend(temp2)\n",
    "    # comment this \"else\" part, if want to include subordinating conjunctions\n",
    "    else:\n",
    "        for i in range(0, num_children):\n",
    "            if t[i].label() in [\"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"]:\n",
    "                temp1, temp2 = get_pos(t[i])\n",
    "                vp_pos.extend(temp1)\n",
    "                sub_conj_pos.extend(temp2)\n",
    "            else:\n",
    "                sub_conj_pos.append(t[i].treeposition())\n",
    "\n",
    "    return (vp_pos,sub_conj_pos)\n",
    "\n",
    "\n",
    "# get all clauses\n",
    "def get_clause_list(sent):\n",
    "    parser = nlp.annotate(sent, properties={\"annotators\":\"parse\",\"outputFormat\": \"json\"})\n",
    "    parser=json.loads(parser)\n",
    "    #print(parser[\"sentences\"][0][\"parse\"])\n",
    "    sent_tree = ParentedTree.fromstring(parser[\"sentences\"][0][\"parse\"])\n",
    "    clause_level_list = [\"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"]\n",
    "    clause_list = []\n",
    "    sub_trees = []\n",
    "    # sent_tree.pretty_print()\n",
    "\n",
    "    # break the tree into subtrees of clauses using\n",
    "    # clause levels \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"\n",
    "    for sub_tree in reversed(list(sent_tree.subtrees())):\n",
    "        #print(sub_tree.label())\n",
    "        if sub_tree.label() in clause_level_list:\n",
    "            if sub_tree.parent().label() in clause_level_list:\n",
    "                continue\n",
    "            if (len(sub_tree) == 1 and sub_tree.label() == \"S\" and sub_tree[0].label() == \"VP\"\n",
    "                and not sub_tree.parent().label() in clause_level_list):\n",
    "                continue\n",
    "\n",
    "            sub_trees.append(sub_tree)\n",
    "            del sent_tree[sub_tree.treeposition()]\n",
    "\n",
    "    # for each clause level subtree, extract relevant simple sentence\n",
    "    for t in sub_trees:\n",
    "        # get verb phrases from the new modified tree\n",
    "        verb_phrases = get_verb_phrases(t)\n",
    "\n",
    "        # get tree without verb phrases (mainly subject)\n",
    "        # remove subordinating conjunctions\n",
    "        vp_pos,sub_conj_pos = get_pos(t)\n",
    "        for i in vp_pos:\n",
    "            del t[i]\n",
    "        for i in sub_conj_pos:\n",
    "            del t[i]\n",
    "        #print(t.label())\n",
    "\n",
    "        # I commented this out so we don't attch the subject to each phrase\n",
    "        subject_phrase = \"\"#' '.join(t.leaves())\n",
    "\n",
    "        # update the clause_list\n",
    "        for i in verb_phrases:\n",
    "            clause_list.append(subject_phrase + \" \" + i)\n",
    "\n",
    "    clause_list.reverse()\n",
    "    return clause_list\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    # sent = \"he plays cricket but does not play hockey\"\n",
    "    # sent = re.sub(r\"(\\.|,|\\?|\\(|\\)|\\[|\\])\",\" \",sent)\n",
    "    # clause_qlist = get_clause_list(sent)\n",
    "    # print(clause_list)\n",
    "#while (True):\n",
    "#sent = input(\"sentence : \\n \")\n",
    "sent = \"Automated inserting and categorizing 20k+ emails from over 200 clients in a CRM by creating an email monitoring REST API using Express, and deployed the service on Azure Cloud with Docker.\"\n",
    "#sent = \"he plays cricket but does not play hockey\"\n",
    "#sent = re.sub(r\"(\\.|,|\\?|\\(|\\)|\\[|\\])\", \" \", sent)\n",
    "clauses = get_clause_list(sent)\n",
    "print(\"num clauses:\", len(clauses))\n",
    "for clause in clauses:\n",
    "    print(\"-\" + clause,end=\"\\n\\n\")\n",
    "#NOTE: sentences must end in a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48b436ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "['he', 'plays', 'cricket', 'but', 'does', 'not', 'play', 'hockey', '.']\n",
      "---------\n",
      "['he', 'plays', 'cricket', 'but', 'does', 'not', 'play', 'hockey', '.']\n",
      "---------\n",
      "['he']\n",
      "---------\n",
      "['he']\n",
      "---------\n",
      "['plays', 'cricket', 'but', 'does', 'not', 'play', 'hockey']\n",
      "---------\n",
      "['plays', 'cricket']\n",
      "---------\n",
      "['plays']\n",
      "---------\n",
      "['cricket']\n",
      "---------\n",
      "['cricket']\n",
      "---------\n",
      "['but']\n",
      "---------\n",
      "['does', 'not', 'play', 'hockey']\n",
      "---------\n",
      "['does']\n",
      "---------\n",
      "['not']\n",
      "---------\n",
      "['play', 'hockey']\n",
      "---------\n",
      "['play']\n",
      "---------\n",
      "['hockey']\n",
      "---------\n",
      "['hockey']\n",
      "---------\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "        # S = Simple declarative clause\n",
    "        # SBar = Subordinate Clause\n",
    "        # SBARQ = Direct question introduced by wh-element\n",
    "        # SINV = Declarative sentence with subject-aux inversion\n",
    "        # SQ = Yes/no questions and subconstituent of SBARQ excluding wh-element\n",
    "sent = \"he plays cricket but does not play hockey.\"\n",
    "parser = nlp.annotate(sent, properties={\"annotators\": \"parse\", \"outputFormat\": \"json\"})\n",
    "parser = json.loads(parser)\n",
    "sent_tree = ParentedTree.fromstring(parser[\"sentences\"][0][\"parse\"])\n",
    "for cur in sent_tree.subtrees():\n",
    "    if cur.label in [\"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\", \"PP\"]:\n",
    "        print(cur.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea3725",
   "metadata": {},
   "source": [
    "input: sentence\n",
    "output: clauses\n",
    "\n",
    "clause:{\n",
    "    verb phrase:\n",
    "    subject:\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0258293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46624a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nlp.annotate(sent, properties={\"annotators\":\"parse\",\"outputFormat\": \"json\"})\n",
    "parser=json.loads(parser)\n",
    "#print(parser[\"sentences\"][0][\"parse\"])\n",
    "sent_tree = ParentedTree.fromstring(parser[\"sentences\"][0][\"parse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28403ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NN hockey)\n",
      "['hockey']\n",
      "(NP (NN hockey))\n",
      "['hockey']\n",
      "(VB play)\n",
      "['play']\n",
      "(VP (VB play) (NP (NN hockey)))\n",
      "['play', 'hockey']\n",
      "(RB not)\n",
      "['not']\n",
      "(VBZ does)\n",
      "['does']\n",
      "(VP (VBZ does) (RB not) (VP (VB play) (NP (NN hockey))))\n",
      "['does', 'not', 'play', 'hockey']\n",
      "(CC but)\n",
      "['but']\n",
      "(NN cricket)\n",
      "['cricket']\n",
      "(NP (NN cricket))\n",
      "['cricket']\n",
      "(VBZ plays)\n",
      "['plays']\n",
      "(VP (VBZ plays) (NP (NN cricket)))\n",
      "['plays', 'cricket']\n",
      "(VP\n",
      "  (VP (VBZ plays) (NP (NN cricket)))\n",
      "  (CC but)\n",
      "  (VP (VBZ does) (RB not) (VP (VB play) (NP (NN hockey)))))\n",
      "['plays', 'cricket', 'but', 'does', 'not', 'play', 'hockey']\n",
      "(PRP he)\n",
      "['he']\n",
      "(NP (PRP he))\n",
      "['he']\n",
      "(S\n",
      "  (NP (PRP he))\n",
      "  (VP\n",
      "    (VP (VBZ plays) (NP (NN cricket)))\n",
      "    (CC but)\n",
      "    (VP (VBZ does) (RB not) (VP (VB play) (NP (NN hockey))))))\n",
      "['he', 'plays', 'cricket', 'but', 'does', 'not', 'play', 'hockey']\n",
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP he))\n",
      "    (VP\n",
      "      (VP (VBZ plays) (NP (NN cricket)))\n",
      "      (CC but)\n",
      "      (VP (VBZ does) (RB not) (VP (VB play) (NP (NN hockey)))))))\n",
      "['he', 'plays', 'cricket', 'but', 'does', 'not', 'play', 'hockey']\n"
     ]
    }
   ],
   "source": [
    "def parse_clauses(sent):\n",
    "    parser = nlp.annotate(sent, properties={\"annotators\":\"parse\",\"outputFormat\": \"json\"})\n",
    "    parser=json.loads(parser)\n",
    "    #print(parser[\"sentences\"][0][\"parse\"])\n",
    "    sent_tree = ParentedTree.fromstring(parser[\"sentences\"][0][\"parse\"])\n",
    "    clause_level_list = [\"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"]\n",
    "    clause_list = []\n",
    "    sub_trees = []\n",
    "    # sent_tree.pretty_print()\n",
    "\n",
    "    # break the tree into subtrees of clauses using\n",
    "    # clause levels \"S\",\"SBAR\",\"SBARQ\",\"SINV\",\"SQ\"\n",
    "    for sub_tree in reversed(list(sent_tree.subtrees())):\n",
    "        print(sub_tree)\n",
    "        print(sub_tree.leaves())\n",
    "    \n",
    "    #if \"VP\" in children and not flag:\n",
    "    #    for i in range(0, num_children):\n",
    "    #        if t[i].label() == \"VP\":\n",
    "    #            vp_pos.append(t[i].treeposition())\n",
    "parse_clauses(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dff2d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['index', 'parse', 'basicDependencies', 'enhancedDependencies', 'enhancedPlusPlusDependencies', 'tokens'])\n",
      "(ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (NNP Automated))\n",
      "      (VP (VBG inserting)\n",
      "        (CC and)\n",
      "        (VBG categorizing)\n",
      "        (NP (JJ 20k) (NNS emails))\n",
      "        (PP (IN from)\n",
      "          (NP\n",
      "            (NP\n",
      "              (QP (RB over) (CD 200))\n",
      "              (NNS clients))\n",
      "            (PP (IN in)\n",
      "              (NP (DT a) (NNP CRM)))))\n",
      "        (PP (IN by)\n",
      "          (S\n",
      "            (VP (VBG creating)\n",
      "              (NP (DT an) (NN email) (NN monitoring)))))))\n",
      "    (VP\n",
      "      (VP (VBD REST)\n",
      "        (NP (NN API))\n",
      "        (S\n",
      "          (VP (VBG using)\n",
      "            (NP (NNP Express)))))\n",
      "      (, ,)\n",
      "      (CC and)\n",
      "      (VP (VBD deployed)\n",
      "        (NP (DT the) (NN service))\n",
      "        (PP (IN on)\n",
      "          (NP\n",
      "            (NP (NNP Azure) (NNP Cloud))\n",
      "            (PP (IN with)\n",
      "              (NP (NNP Docker)))))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "print(parser[\"sentences\"][0].keys())\n",
    "print(parser[\"sentences\"][0][\"parse\"])\n",
    "#print(sent_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
